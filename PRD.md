PRD – CSV Upload & LLM Preprocessing Interface

1. Overview

We are building a web-based tool that enables users to:

Upload CSV files.

Display the uploaded CSV contents on the interface.

Select a target column for prediction via a dropdown menu.

Trigger a Start LLM Pipeline button that runs data preprocessing and feature selection using an LLM-generated Python pipeline.

Store uploaded CSVs in AWS S3.

Backend will handle preprocessing, code generation, and execution.

This product combines a React + TailwindCSS frontend with a Flask backend and integrates with:

Claude (LLM for code generation).

E2b (for executing generated Python preprocessing code).

AWS S3 (for file storage).

2. Goals & Non-Goals
   Goals

Simplify uploading and previewing CSVs.

Let users select a column for prediction.

Automate preprocessing (handle nulls, remove outliers, standardize data).

Run preprocessing code generated by Claude on uploaded data.

Store files safely in S3.

Non-Goals

Full model training (only preprocessing and column selection now).

Supporting file types other than .csv.

Advanced visualization (basic table display only).

3. User Flow

Upload CSV

User uploads .csv file.

File is validated, stored in S3, and previewed in the UI.

Select Prediction Column

Dropdown lists column names from CSV.

User selects one column.

Start LLM Pipeline

User clicks button.

Backend sends preprocessing request to Claude.

Claude generates Python code for handling nulls, outliers, and standardization.

Flask executes generated code via E2b.

Cleaned dataset (minus prediction column) is prepared.

4. Functional Requirements
   Frontend (React + TailwindCSS)

File Upload Component

Restrict file type: .csv only.

Upload progress indicator.

CSV Table Viewer

Display first 50 rows of the CSV for preview.

Dropdown Menu

Populate with column names from CSV header.

Start LLM Pipeline Button

Disabled until column is selected.

Shows loading state when running pipeline.

Result Display

Show confirmation once preprocessing completes.

Backend (Flask)

File Handling

Receive uploaded file.

Validate .csv.

Store in AWS S3 with unique filename.

Preprocessing Pipeline

Input: CSV + target column.

Step 1: Send description + sample of CSV + requirements to Claude.

Step 2: Claude outputs Python preprocessing script:

Handle missing values.

Detect/remove outliers.

Standardize numerical features.

Drop prediction column.

Step 3: Execute script using E2b.

Step 4: Return confirmation + cleaned dataset.

API Endpoints

POST /upload → Upload CSV to S3 and return preview + headers.

POST /preprocess → Run preprocessing pipeline, return status + cleaned dataset summary.

5. Technical Architecture

Frontend

React + TailwindCSS.

Axios for API calls.

CSV preview via papaparse.

Backend

Flask (REST API).

AWS SDK (boto3) for S3 integration.

Claude API for code generation.

E2b for safe code execution.

Infrastructure

AWS S3 for file storage.

AWS Lambda or Flask server for hosting backend.

Docker for containerization (optional).

6. Data Flow

User uploads CSV → React → Flask /upload → S3.

Flask parses CSV header → returns preview + headers.

User selects target column → sends request /preprocess.

Flask sends CSV sample + instructions → Claude.

Claude returns Python script.

Flask runs script via E2b → outputs cleaned dataset.

Flask returns confirmation to frontend.

7. Success Metrics

95%+ of valid CSV uploads succeed.

Preprocessing completes within 30 seconds for datasets < 50MB.

Users successfully select and run pipeline without errors.

8. Future Enhancements

Add visualization (charts, distributions).

Allow multiple preprocessing options.

Add model training (e.g., regression/classification).

Support larger file uploads (chunked S3 uploads
